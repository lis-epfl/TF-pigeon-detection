{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/leocencetti/TF_object_detection/blob/master/TF_object_detection_training_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yhzxsJb3dpWq"
   },
   "source": [
    "## Configs and Hyperparameters\n",
    "\n",
    "Support a variety of models, you can find more pretrained model from [Tensorflow detection model zoo: COCO-trained models](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md#coco-trained-models), as well as their pipline config files in [object_detection/samples/configs/](https://github.com/tensorflow/models/tree/master/research/object_detection/samples/configs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gnNXNQCjdniL"
   },
   "outputs": [],
   "source": [
    "# Repo link.\n",
    "repo_url = 'https://github.com/leocencetti/TF_object_detection'\n",
    "\n",
    "# Number of training steps.\n",
    "num_steps = 40000  # 200000\n",
    "\n",
    "# Number of evaluation steps.\n",
    "num_eval_steps = 500\n",
    "\n",
    "MODELS_CONFIG = {\n",
    "    'ssd_mobilenet_v2': {\n",
    "        'model_name': 'ssd_mobilenet_v2_coco_2018_03_29',\n",
    "        'pipeline_file': 'ssd_mobilenet_v2_coco.config',\n",
    "        'batch_size': 11\n",
    "    },\n",
    "    'faster_rcnn_inception_v2': {\n",
    "        'model_name': 'faster_rcnn_inception_v2_coco_2018_01_28',\n",
    "        'pipeline_file': 'faster_rcnn_inception_v2_pets.config',\n",
    "        'batch_size': 12\n",
    "    },\n",
    "    'rfcn_resnet101': {\n",
    "        'model_name': 'rfcn_resnet101_coco_2018_01_28',\n",
    "        'pipeline_file': 'rfcn_resnet101_pets.config',\n",
    "        'batch_size': 8\n",
    "    }\n",
    "}\n",
    "\n",
    "# Pick the model you want to use\n",
    "# Select a model in `MODELS_CONFIG`.\n",
    "selected_model = 'faster_rcnn_inception_v2'\n",
    "\n",
    "# Name of the object detection model to use.\n",
    "MODEL = MODELS_CONFIG[selected_model]['model_name']\n",
    "\n",
    "# Name of the pipline file in tensorflow object detection API.\n",
    "pipeline_file = MODELS_CONFIG[selected_model]['pipeline_file']\n",
    "\n",
    "# Training batch size fits in Colabe's Tesla K80 GPU memory for selected model.\n",
    "batch_size = MODELS_CONFIG[selected_model]['batch_size']\n",
    "\n",
    "# Training input tensor size\n",
    "img_height = 544\n",
    "img_width = 960"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w4V-XE6kbkc1"
   },
   "source": [
    "## Clone the `TF_object_detection` repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dxc3DmvLQF3z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '/content'\n",
      "/home/TF-pigeon-detection\n",
      "/bin/sh: 1: git: not found\n",
      "[Errno 2] No such file or directory: '/home/TF-pigeon-detection/TF_object_detection'\n",
      "/home/TF-pigeon-detection\n",
      "/bin/sh: 1: git: not found\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "%cd /content\n",
    "\n",
    "repo_dir_path = os.path.abspath(os.path.join('.', os.path.basename(repo_url)))\n",
    "\n",
    "!git clone {repo_url}\n",
    "%cd {repo_dir_path}\n",
    "!git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "libsm6 is already the newest version (2:1.2.2-1).\n",
      "libxext6 is already the newest version (2:1.3.3-1).\n",
      "libxrender-dev is already the newest version (1:0.9.10-1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 13 not upgraded.\n",
      "Collecting opencv-python\n",
      "  Using cached https://files.pythonhosted.org/packages/5e/7e/bd5425f4dacb73367fddc71388a47c1ea570839197c2bcad86478e565186/opencv_python-4.1.1.26-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting numpy>=1.11.3 (from opencv-python)\n",
      "  Using cached https://files.pythonhosted.org/packages/e5/e6/c3fdc53aed9fa19d6ff3abf97dfad768ae3afce1b7431f7500000816bda5/numpy-1.17.2-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Installing collected packages: numpy, opencv-python\n",
      "Successfully installed numpy-1.17.2 opencv-python-4.1.1.26\n"
     ]
    }
   ],
   "source": [
    "!apt-get install -y libsm6 libxext6 libxrender-dev\n",
    "!pip install opencv-python --ignore-installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: progressbar in /usr/local/lib/python3.6/dist-packages (2.5)\n",
      "Collecting glob3\n",
      "  Downloading https://files.pythonhosted.org/packages/d6/21/e042ea7bcb917bac5cf2365d532d44efc23650be81456546a6e89e25371e/glob3-0.0.1.tar.gz\n",
      "Building wheels for collected packages: glob3\n",
      "  Building wheel for glob3 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for glob3: filename=glob3-0.0.1-cp36-none-any.whl size=2411 sha256=269d64b53ee52c228b85c603ac44fad90349437fcd2a8f1e2de7f9436c47ebde\n",
      "  Stored in directory: /root/.cache/pip/wheels/5c/db/36/aae4683ac0ba95eb154510b48d6bda87fbaac71b6a9b62123d\n",
      "Successfully built glob3\n",
      "Installing collected packages: glob3\n",
      "Successfully installed glob3-0.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install progressbar2 glob3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HcHqdWBNZdcr",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Try to fix images (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sYR3QDZ4Zdcs",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import progressbar\n",
    "import glob\n",
    "\n",
    "img_dirs = ['/home/TF_object_detection/data/images/test', '/home/TF_object_detection/data/images/train']\n",
    "for dir in img_dirs:\n",
    "    os.chdir(dir)\n",
    "    for img in glob.glob('*.jpg'):\n",
    "        cv2.imwrite(img, cv2.imread(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bI8__uNS8-ns"
   },
   "source": [
    "## Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ecpHEnka8Kix"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: Failed to fetch http://archive.ubuntu.com/ubuntu/pool/main/p/python2.7/libpython2.7-minimal_2.7.15-4ubuntu4~18.04_amd64.deb  404  Not Found [IP: 91.189.88.173 80]\n",
      "E: Failed to fetch http://archive.ubuntu.com/ubuntu/pool/main/p/python2.7/python2.7-minimal_2.7.15-4ubuntu4~18.04_amd64.deb  404  Not Found [IP: 91.189.88.173 80]\n",
      "E: Failed to fetch http://archive.ubuntu.com/ubuntu/pool/main/p/python2.7/libpython2.7-stdlib_2.7.15-4ubuntu4~18.04_amd64.deb  404  Not Found [IP: 91.189.88.173 80]\n",
      "E: Failed to fetch http://archive.ubuntu.com/ubuntu/pool/main/p/python2.7/python2.7_2.7.15-4ubuntu4~18.04_amd64.deb  404  Not Found [IP: 91.189.88.173 80]\n",
      "E: Failed to fetch http://security.ubuntu.com/ubuntu/pool/main/t/tzdata/tzdata_2019a-0ubuntu0.18.04_all.deb  404  Not Found [IP: 91.189.88.173 80]\n",
      "E: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?\n",
      "/home/TensorFlow/models/research\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1008 14:38:53.763917 140175704483648 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W1008 14:38:53.916884 140175704483648 deprecation_wrapper.py:119] From /home/TensorFlow/models/research/slim/nets/inception_resnet_v2.py:373: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "W1008 14:38:53.941292 140175704483648 deprecation_wrapper.py:119] From /home/TensorFlow/models/research/slim/nets/mobilenet/mobilenet.py:397: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n",
      "Running tests under Python 3.6.8: /usr/local/bin/python\n",
      "[ RUN      ] ModelBuilderTest.test_create_faster_rcnn_model_from_config_with_example_miner\n",
      "[       OK ] ModelBuilderTest.test_create_faster_rcnn_model_from_config_with_example_miner\n",
      "[ RUN      ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
      "[       OK ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
      "[ RUN      ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
      "[       OK ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
      "[ RUN      ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
      "[       OK ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
      "[ RUN      ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n",
      "[       OK ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n",
      "[ RUN      ] ModelBuilderTest.test_create_rfcn_model_from_config\n",
      "[       OK ] ModelBuilderTest.test_create_rfcn_model_from_config\n",
      "[ RUN      ] ModelBuilderTest.test_create_ssd_fpn_model_from_config\n",
      "[       OK ] ModelBuilderTest.test_create_ssd_fpn_model_from_config\n",
      "[ RUN      ] ModelBuilderTest.test_create_ssd_models_from_config\n",
      "[       OK ] ModelBuilderTest.test_create_ssd_models_from_config\n",
      "[ RUN      ] ModelBuilderTest.test_invalid_faster_rcnn_batchnorm_update\n",
      "[       OK ] ModelBuilderTest.test_invalid_faster_rcnn_batchnorm_update\n",
      "[ RUN      ] ModelBuilderTest.test_invalid_first_stage_nms_iou_threshold\n",
      "[       OK ] ModelBuilderTest.test_invalid_first_stage_nms_iou_threshold\n",
      "[ RUN      ] ModelBuilderTest.test_invalid_model_config_proto\n",
      "[       OK ] ModelBuilderTest.test_invalid_model_config_proto\n",
      "[ RUN      ] ModelBuilderTest.test_invalid_second_stage_batch_size\n",
      "[       OK ] ModelBuilderTest.test_invalid_second_stage_batch_size\n",
      "[ RUN      ] ModelBuilderTest.test_session\n",
      "[  SKIPPED ] ModelBuilderTest.test_session\n",
      "[ RUN      ] ModelBuilderTest.test_unknown_faster_rcnn_feature_extractor\n",
      "[       OK ] ModelBuilderTest.test_unknown_faster_rcnn_feature_extractor\n",
      "[ RUN      ] ModelBuilderTest.test_unknown_meta_architecture\n",
      "[       OK ] ModelBuilderTest.test_unknown_meta_architecture\n",
      "[ RUN      ] ModelBuilderTest.test_unknown_ssd_feature_extractor\n",
      "[       OK ] ModelBuilderTest.test_unknown_ssd_feature_extractor\n",
      "----------------------------------------------------------------------\n",
      "Ran 16 tests in 0.164s\n",
      "\n",
      "OK (skipped=1)\n"
     ]
    }
   ],
   "source": [
    "# %cd /content\n",
    "# !git clone --quiet https://github.com/tensorflow/models.git\n",
    "\n",
    "!apt-get install -qq protobuf-compiler python-pil python-lxml python-tk\n",
    "\n",
    "!pip install -q Cython contextlib2 pillow lxml matplotlib\n",
    "\n",
    "!pip install -q pycocotools\n",
    "\n",
    "%cd /home/TensorFlow/models/research\n",
    "!protoc object_detection/protos/*.proto --python_out=.\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONPATH'] += ':/home/TensorFlow/models/research/:/home/TensorFlow/models/research/slim/'\n",
    "\n",
    "!python object_detection/builders/model_builder_test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u-k7uGThXlny"
   },
   "source": [
    "## Prepare `tfrecord` files\n",
    "\n",
    "Use the following scripts to generate the `tfrecord` files.\n",
    "```bash\n",
    "# Convert train folder annotation xml files to a single csv file,\n",
    "# generate the `label_map.pbtxt` file to `data/` directory as well.\n",
    "python xml_to_csv.py -i data/images/train -o data/annotations/train_labels.csv -l data/annotations\n",
    "\n",
    "# Convert test folder annotation xml files to a single csv.\n",
    "python xml_to_csv.py -i data/images/test -o data/annotations/test_labels.csv\n",
    "\n",
    "# Generate `train.record`\n",
    "python generate_tfrecord.py --csv_input=data/annotations/train_labels.csv --output_path=data/annotations/train.record --img_path=data/images/train --label_map data/annotations/label_map.pbtxt\n",
    "\n",
    "# Generate `test.record`\n",
    "python generate_tfrecord.py --csv_input=data/annotations/test_labels.csv --output_path=data/annotations/test.record --img_path=data/images/test --label_map data/annotations/label_map.pbtxt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ezGDABRXXhPP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/TF_object_detection\n",
      "Successfully converted xml to csv.\n",
      "Generate `data/annotations/label_map.pbtxt`\n",
      "Successfully converted xml to csv.\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "W0913 11:03:10.635046 140186697754432 deprecation_wrapper.py:119] From /home/TensorFlow/models/research/object_detection/utils/label_map_util.py:137: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "Successfully created the TFRecords: /home/TF_object_detection/data/annotations/train.record\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "W0913 11:03:15.996063 140089542055744 deprecation_wrapper.py:119] From /home/TensorFlow/models/research/object_detection/utils/label_map_util.py:137: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "Successfully created the TFRecords: /home/TF_object_detection/data/annotations/test.record\n"
     ]
    }
   ],
   "source": [
    "repo_dir_path = '/home/TF_object_detection/'\n",
    "%cd {repo_dir_path}\n",
    "\n",
    "# Convert train folder annotation xml files to a single csv file,\n",
    "# generate the `label_map.pbtxt` file to `data/` directory as well.\n",
    "!python xml_to_csv.py -i data/images/train -o data/annotations/train_labels.csv -l data/annotations\n",
    "\n",
    "# Convert test folder annotation xml files to a single csv.\n",
    "!python xml_to_csv.py -i data/images/test -o data/annotations/test_labels.csv\n",
    "\n",
    "# Generate `train.record`\n",
    "!python generate_tfrecord.py --csv_input=data/annotations/train_labels.csv --output_path=data/annotations/train.record --img_path=data/images/train --label_map data/annotations/label_map.pbtxt\n",
    "\n",
    "# Generate `test.record`\n",
    "!python generate_tfrecord.py --csv_input=data/annotations/test_labels.csv --output_path=data/annotations/test.record --img_path=data/images/test --label_map data/annotations/label_map.pbtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tgd-fzAIkZlV"
   },
   "outputs": [],
   "source": [
    "test_record_fname = '/home/TF-pigeon-detection/data/annotations/test.record'\n",
    "train_record_fname = '/home/TF-pigeon-detection/data/annotations/train.record'\n",
    "label_map_pbtxt_fname = '/home/TF-pigeon-detection/data/annotations/label_map.pbtxt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iCNYAaC7w6N8"
   },
   "source": [
    "## Download base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "orDCj6ihgUMR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/TensorFlow/models/research\n"
     ]
    }
   ],
   "source": [
    "%cd /home/TensorFlow/models/research\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import urllib.request\n",
    "import tarfile\n",
    "MODEL_FILE = MODEL + '.tar.gz'\n",
    "DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n",
    "DEST_DIR = '/home/TensorFlow/models/research/pretrained_model'\n",
    "\n",
    "if not (os.path.exists(MODEL_FILE)):\n",
    "    urllib.request.urlretrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\n",
    "\n",
    "tar = tarfile.open(MODEL_FILE)\n",
    "tar.extractall()\n",
    "tar.close()\n",
    "\n",
    "os.remove(MODEL_FILE)\n",
    "if (os.path.exists(DEST_DIR)):\n",
    "    shutil.rmtree(DEST_DIR)\n",
    "os.rename(MODEL, DEST_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pGhvAObeiIix"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/TensorFlow/models/research/pretrained_model\n",
      "total 111M\n",
      "drwxr-xr-x  3 345018 5000 4.0K Feb  1  2018 .\n",
      "drwxr-xr-x 75   1000 1000 4.0K Oct  8 14:40 ..\n",
      "-rw-r--r--  1 345018 5000   77 Feb  1  2018 checkpoint\n",
      "-rw-r--r--  1 345018 5000  55M Feb  1  2018 frozen_inference_graph.pb\n",
      "-rw-r--r--  1 345018 5000  51M Feb  1  2018 model.ckpt.data-00000-of-00001\n",
      "-rw-r--r--  1 345018 5000  16K Feb  1  2018 model.ckpt.index\n",
      "-rw-r--r--  1 345018 5000 5.5M Feb  1  2018 model.ckpt.meta\n",
      "-rw-r--r--  1 345018 5000 3.2K Feb  1  2018 pipeline.config\n",
      "drwxr-xr-x  3 345018 5000 4.0K Feb  1  2018 saved_model\n"
     ]
    }
   ],
   "source": [
    "!echo {DEST_DIR}\n",
    "!ls -alh {DEST_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UHnxlfRznPP3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/TensorFlow/models/research/pretrained_model/model.ckpt'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_tune_checkpoint = os.path.join(DEST_DIR, \"model.ckpt\")\n",
    "fine_tune_checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MvwtHlLOeRJD"
   },
   "source": [
    "## Configuring a Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dIhw7IdpLuiU"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "pipeline_fname = os.path.join('/home/TensorFlow/models/research/object_detection/samples/configs/', pipeline_file)\n",
    "# pipeline_fname = '/home/TF_object_detection/adam_pipeline.config'\n",
    "assert os.path.isfile(pipeline_fname), '`{}` not exist'.format(pipeline_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fG1nCNpUXcRU"
   },
   "outputs": [],
   "source": [
    "def get_num_classes(pbtxt_fname):\n",
    "    from object_detection.utils import label_map_util\n",
    "    label_map = label_map_util.load_labelmap(pbtxt_fname)\n",
    "    categories = label_map_util.convert_label_map_to_categories(\n",
    "        label_map, max_num_classes=90, use_display_name=True)\n",
    "    category_index = label_map_util.create_category_index(categories)\n",
    "    return len(category_index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YjtCbLF2i0wI",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "num_classes = get_num_classes(label_map_pbtxt_fname)\n",
    "with open(pipeline_fname) as f:\n",
    "    s = f.read()\n",
    "with open(pipeline_fname, 'w') as f:\n",
    "    \n",
    "    # fine_tune_checkpoint\n",
    "    s = re.sub('fine_tune_checkpoint: \".*?\"',\n",
    "               'fine_tune_checkpoint: \"{}\"'.format(fine_tune_checkpoint), s)\n",
    "    \n",
    "    # tfrecord files train and test.\n",
    "    s = re.sub(\n",
    "        '(input_path: \".*?)(train.record)(.*?\")', 'input_path: \"{}\"'.format(train_record_fname), s)\n",
    "    s = re.sub(\n",
    "        '(input_path: \".*?)(val.record)(.*?\")', 'input_path: \"{}\"'.format(test_record_fname), s)\n",
    "\n",
    "    # label_map_path\n",
    "    s = re.sub(\n",
    "        'label_map_path: \".*?\"', 'label_map_path: \"{}\"'.format(label_map_pbtxt_fname), s)\n",
    "\n",
    "    # Set training batch_size.\n",
    "    s = re.sub('batch_size: [0-9]+',\n",
    "               'batch_size: {}'.format(batch_size), s)\n",
    "\n",
    "    # Set training steps, num_steps\n",
    "    s = re.sub('num_steps: [0-9]+',\n",
    "               'num_steps: {}'.format(num_steps), s)\n",
    "    \n",
    "    # Set number of classes num_classes.\n",
    "    s = re.sub('num_classes: [0-9]+',\n",
    "               'num_classes: {}'.format(num_classes), s)\n",
    "    \n",
    "#     # Set the input tensor size\n",
    "    s = re.sub('keep_aspect_ratio_resizer', 'fixed_shape_resizer', s)\n",
    "    s = re.sub('min_dimension: [0-9]+', 'height: {}'.format(600), s)\n",
    "    s = re.sub('max_dimension: [0-9]+', 'width: {}'.format(1024), s)\n",
    "    \n",
    "    # Set the example number\n",
    "    s = re.sub('num_examples: [0-9]+', 'num_examples: 4000', s)\n",
    "\n",
    "    f.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GH0MEEanocn6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Faster R-CNN with Inception v2, configured for Oxford-IIIT Pets Dataset.\r\n",
      "# Users should configure the fine_tune_checkpoint field in the train config as\r\n",
      "# well as the label_map_path and input_path fields in the train_input_reader and\r\n",
      "# eval_input_reader. Search for \"PATH_TO_BE_CONFIGURED\" to find the fields that\r\n",
      "# should be configured.\r\n",
      "\r\n",
      "model {\r\n",
      "  faster_rcnn {\r\n",
      "    num_classes: 2\r\n",
      "    image_resizer {\r\n",
      "      fixed_shape_resizer {\r\n",
      "        height: 600\r\n",
      "        width: 1024\r\n",
      "      }\r\n",
      "    }\r\n",
      "    feature_extractor {\r\n",
      "      type: 'faster_rcnn_inception_v2'\r\n",
      "      first_stage_features_stride: 16\r\n",
      "    }\r\n",
      "    first_stage_anchor_generator {\r\n",
      "      grid_anchor_generator {\r\n",
      "        scales: [0.25, 0.5, 1.0, 2.0]\r\n",
      "        aspect_ratios: [0.5, 1.0, 2.0]\r\n",
      "        height_stride: 16\r\n",
      "        width_stride: 16\r\n",
      "      }\r\n",
      "    }\r\n",
      "    first_stage_box_predictor_conv_hyperparams {\r\n",
      "      op: CONV\r\n",
      "      regularizer {\r\n",
      "        l2_regularizer {\r\n",
      "          weight: 0.0\r\n",
      "        }\r\n",
      "      }\r\n",
      "      initializer {\r\n",
      "        truncated_normal_initializer {\r\n",
      "          stddev: 0.01\r\n",
      "        }\r\n",
      "      }\r\n",
      "    }\r\n",
      "    first_stage_nms_score_threshold: 0.0\r\n",
      "    first_stage_nms_iou_threshold: 0.7\r\n",
      "    first_stage_max_proposals: 300\r\n",
      "    first_stage_localization_loss_weight: 2.0\r\n",
      "    first_stage_objectness_loss_weight: 1.0\r\n",
      "    initial_crop_size: 14\r\n",
      "    maxpool_kernel_size: 2\r\n",
      "    maxpool_stride: 2\r\n",
      "    second_stage_box_predictor {\r\n",
      "      mask_rcnn_box_predictor {\r\n",
      "        use_dropout: false\r\n",
      "        dropout_keep_probability: 1.0\r\n",
      "        fc_hyperparams {\r\n",
      "          op: FC\r\n",
      "          regularizer {\r\n",
      "            l2_regularizer {\r\n",
      "              weight: 0.0\r\n",
      "            }\r\n",
      "          }\r\n",
      "          initializer {\r\n",
      "            variance_scaling_initializer {\r\n",
      "              factor: 1.0\r\n",
      "              uniform: true\r\n",
      "              mode: FAN_AVG\r\n",
      "            }\r\n",
      "          }\r\n",
      "        }\r\n",
      "      }\r\n",
      "    }\r\n",
      "    second_stage_post_processing {\r\n",
      "      batch_non_max_suppression {\r\n",
      "        score_threshold: 0.0\r\n",
      "        iou_threshold: 0.6\r\n",
      "        max_detections_per_class: 100\r\n",
      "        max_total_detections: 300\r\n",
      "      }\r\n",
      "      score_converter: SOFTMAX\r\n",
      "    }\r\n",
      "    second_stage_localization_loss_weight: 2.0\r\n",
      "    second_stage_classification_loss_weight: 1.0\r\n",
      "  }\r\n",
      "}\r\n",
      "\r\n",
      "train_config: {\r\n",
      "  batch_size: 12\r\n",
      "  optimizer {\r\n",
      "    momentum_optimizer: {\r\n",
      "      learning_rate: {\r\n",
      "        manual_step_learning_rate {\r\n",
      "          initial_learning_rate: 0.0002\r\n",
      "          schedule {\r\n",
      "            step: 900000\r\n",
      "            learning_rate: .00002\r\n",
      "          }\r\n",
      "          schedule {\r\n",
      "            step: 1200000\r\n",
      "            learning_rate: .000002\r\n",
      "          }\r\n",
      "        }\r\n",
      "      }\r\n",
      "      momentum_optimizer_value: 0.9\r\n",
      "    }\r\n",
      "    use_moving_average: false\r\n",
      "  }\r\n",
      "  gradient_clipping_by_norm: 10.0\r\n",
      "  fine_tune_checkpoint: \"/home/TensorFlow/models/research/pretrained_model/model.ckpt\"\r\n",
      "  from_detection_checkpoint: true\r\n",
      "  load_all_detection_checkpoint_vars: true\r\n",
      "  # Note: The below line limits the training process to 200K steps, which we\r\n",
      "  # empirically found to be sufficient enough to train the pets dataset. This\r\n",
      "  # effectively bypasses the learning rate schedule (the learning rate will\r\n",
      "  # never decay). Remove the below line to train indefinitely.\r\n",
      "  num_steps: 40000\r\n",
      "  data_augmentation_options {\r\n",
      "    random_horizontal_flip {\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "\r\n",
      "\r\n",
      "train_input_reader: {\r\n",
      "  tf_record_input_reader {\r\n",
      "    input_path: \"/home/TF-pigeon-detection/data/annotations/train.record\"\r\n",
      "  }\r\n",
      "  label_map_path: \"/home/TF-pigeon-detection/data/annotations/label_map.pbtxt\"\r\n",
      "}\r\n",
      "\r\n",
      "eval_config: {\r\n",
      "  metrics_set: \"coco_detection_metrics\"\r\n",
      "  num_examples: 4000\r\n",
      "}\r\n",
      "\r\n",
      "eval_input_reader: {\r\n",
      "  tf_record_input_reader {\r\n",
      "    input_path: \"/home/TF_object_detection/data/annotations/test.record\"\r\n",
      "  }\r\n",
      "  label_map_path: \"/home/TF-pigeon-detection/data/annotations/label_map.pbtxt\"\r\n",
      "  shuffle: false\r\n",
      "  num_readers: 1\r\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat {pipeline_fname}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f11w0uO3jFCB"
   },
   "outputs": [],
   "source": [
    "# model_dir = 'training/'\n",
    "model_dir = '/home/FasterRCNN_inV2_pigeon_test/training/'\n",
    "# Optionally remove content in output model directory to fresh start.\n",
    "!rm -rf \"{model_dir}\"\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JDddx2rPfex9"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CjDHjhKQofT5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1008 14:43:32.029000 140396949854016 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W1008 14:43:32.063321 140396949854016 deprecation_wrapper.py:119] From /home/TensorFlow/models/research/slim/nets/inception_resnet_v2.py:373: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "W1008 14:43:32.071362 140396949854016 deprecation_wrapper.py:119] From /home/TensorFlow/models/research/slim/nets/mobilenet/mobilenet.py:397: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n",
      "W1008 14:43:32.102100 140396949854016 deprecation_wrapper.py:119] From /home/TensorFlow/models/research/object_detection/model_main.py:109: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
      "\n",
      "W1008 14:43:32.102602 140396949854016 deprecation_wrapper.py:119] From /home/TensorFlow/models/research/object_detection/utils/config_util.py:102: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "W1008 14:43:32.104963 140396949854016 deprecation_wrapper.py:119] From /home/TensorFlow/models/research/object_detection/model_lib.py:616: The name tf.logging.warning is deprecated. Please use tf.compat.v1.logging.warning instead.\n",
      "\n",
      "W1008 14:43:32.105038 140396949854016 model_lib.py:617] Forced number of epochs for all eval validations to be 1.\n",
      "W1008 14:43:32.105111 140396949854016 deprecation_wrapper.py:119] From /home/TensorFlow/models/research/object_detection/utils/config_util.py:488: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n",
      "I1008 14:43:32.105159 140396949854016 config_util.py:488] Maybe overwriting train_steps: 40000\n",
      "I1008 14:43:32.105210 140396949854016 config_util.py:488] Maybe overwriting use_bfloat16: False\n",
      "I1008 14:43:32.105252 140396949854016 config_util.py:488] Maybe overwriting sample_1_of_n_eval_examples: 1\n",
      "I1008 14:43:32.105298 140396949854016 config_util.py:488] Maybe overwriting eval_num_epochs: 1\n",
      "I1008 14:43:32.105338 140396949854016 config_util.py:488] Maybe overwriting load_pretrained: True\n",
      "I1008 14:43:32.105378 140396949854016 config_util.py:498] Ignoring config override key: load_pretrained\n",
      "W1008 14:43:32.105857 140396949854016 model_lib.py:633] Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.\n",
      "I1008 14:43:32.105916 140396949854016 model_lib.py:668] create_estimator_and_inputs: use_tpu False, export_to_tpu False\n",
      "I1008 14:43:32.106442 140396949854016 estimator.py:209] Using config: {'_model_dir': '/home/FasterRCNN_inV2_pigeon_test/training/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fb047a36390>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "W1008 14:43:32.106619 140396949854016 model_fn.py:630] Estimator's model_fn (<function create_model_fn.<locals>.model_fn at 0x7fb047a40488>) includes params argument, but params are not passed to Estimator.\n",
      "I1008 14:43:32.107276 140396949854016 estimator_training.py:186] Not using Distribute Coordinator.\n",
      "I1008 14:43:32.107393 140396949854016 training.py:612] Running training and evaluation locally (non-distributed).\n",
      "I1008 14:43:32.107551 140396949854016 training.py:700] Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n",
      "W1008 14:43:32.113528 140396949854016 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1008 14:43:32.123073 140396949854016 deprecation_wrapper.py:119] From /home/TensorFlow/models/research/object_detection/data_decoders/tf_example_decoder.py:182: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
      "\n",
      "W1008 14:43:32.123361 140396949854016 deprecation_wrapper.py:119] From /home/TensorFlow/models/research/object_detection/data_decoders/tf_example_decoder.py:197: The name tf.VarLenFeature is deprecated. Please use tf.io.VarLenFeature instead.\n",
      "\n",
      "W1008 14:43:32.138255 140396949854016 deprecation_wrapper.py:119] From /home/TensorFlow/models/research/object_detection/builders/dataset_builder.py:64: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.\n",
      "\n",
      "W1008 14:43:32.140429 140396949854016 dataset_builder.py:72] num_readers has been reduced to 1 to match input file shards.\n",
      "W1008 14:43:32.147143 140396949854016 deprecation.py:323] From /home/TensorFlow/models/research/object_detection/builders/dataset_builder.py:86: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.experimental.parallel_interleave(...)`.\n",
      "W1008 14:43:32.147728 140396949854016 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/data/python/ops/interleave_ops.py:77: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
      "W1008 14:43:32.177531 140396949854016 deprecation.py:323] From /home/TensorFlow/models/research/object_detection/builders/dataset_builder.py:155: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map()\n",
      "W1008 14:43:32.333318 140396949854016 deprecation_wrapper.py:119] From /home/TensorFlow/models/research/object_detection/utils/ops.py:491: The name tf.is_nan is deprecated. Please use tf.math.is_nan instead.\n",
      "\n",
      "W1008 14:43:32.336087 140396949854016 deprecation.py:323] From /home/TensorFlow/models/research/object_detection/utils/ops.py:493: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W1008 14:43:32.375155 140396949854016 deprecation_wrapper.py:119] From /home/TensorFlow/models/research/object_detection/core/preprocessor.py:626: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1008 14:43:32.408276 140396949854016 deprecation_wrapper.py:119] From /home/TensorFlow/models/research/object_detection/core/preprocessor.py:2660: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n",
      "\n",
      "W1008 14:43:32.741233 140396949854016 deprecation.py:323] From /home/TensorFlow/models/research/object_detection/builders/dataset_builder.py:158: batch_and_drop_remainder (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.batch(..., drop_remainder=True)`.\n",
      "I1008 14:43:32.750238 140396949854016 estimator.py:1145] Calling model_fn.\n",
      "W1008 14:43:32.894735 140396949854016 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "I1008 14:43:34.700902 140396949854016 regularizers.py:98] Scale of 0 disables regularizer.\n",
      "I1008 14:43:34.712932 140396949854016 regularizers.py:98] Scale of 0 disables regularizer.\n",
      "I1008 14:43:34.713278 140396949854016 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
      "W1008 14:43:40.562177 140396949854016 deprecation.py:506] From /home/TensorFlow/models/research/object_detection/utils/spatial_transform_ops.py:418: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "box_ind is deprecated, use box_indices instead\n",
      "W1008 14:43:40.988859 140396949854016 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "I1008 14:43:41.136289 140396949854016 regularizers.py:98] Scale of 0 disables regularizer.\n",
      "I1008 14:43:41.337622 140396949854016 regularizers.py:98] Scale of 0 disables regularizer.\n",
      "W1008 14:43:41.397145 140396949854016 deprecation.py:323] From /home/TensorFlow/models/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py:2650: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.get_or_create_global_step\n",
      "W1008 14:43:41.398373 140396949854016 deprecation_wrapper.py:119] From /home/TensorFlow/models/research/object_detection/utils/variables_helper.py:139: The name tf.train.NewCheckpointReader is deprecated. Please use tf.compat.v1.train.NewCheckpointReader instead.\n",
      "\n",
      "W1008 14:43:41.404175 140396949854016 variables_helper.py:154] Variable [SecondStageBoxPredictor/BoxEncodingPredictor/biases] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[360]], model variable shape: [[8]]. This variable will not be initialized from the checkpoint.\n",
      "W1008 14:43:41.404300 140396949854016 variables_helper.py:154] Variable [SecondStageBoxPredictor/BoxEncodingPredictor/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1024, 360]], model variable shape: [[1024, 8]]. This variable will not be initialized from the checkpoint.\n",
      "W1008 14:43:41.404362 140396949854016 variables_helper.py:154] Variable [SecondStageBoxPredictor/ClassPredictor/biases] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[91]], model variable shape: [[3]]. This variable will not be initialized from the checkpoint.\n",
      "W1008 14:43:41.404412 140396949854016 variables_helper.py:154] Variable [SecondStageBoxPredictor/ClassPredictor/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1024, 91]], model variable shape: [[1024, 3]]. This variable will not be initialized from the checkpoint.\n",
      "W1008 14:43:41.405014 140396949854016 variables_helper.py:157] Variable [global_step] is not available in checkpoint\n",
      "W1008 14:43:41.405139 140396949854016 deprecation_wrapper.py:119] From /home/TensorFlow/models/research/object_detection/model_lib.py:345: The name tf.train.init_from_checkpoint is deprecated. Please use tf.compat.v1.train.init_from_checkpoint instead.\n",
      "\n",
      "W1008 14:43:44.346560 140396949854016 deprecation_wrapper.py:119] From /home/TensorFlow/models/research/object_detection/core/losses.py:177: The name tf.losses.huber_loss is deprecated. Please use tf.compat.v1.losses.huber_loss instead.\n",
      "\n",
      "W1008 14:43:44.347435 140396949854016 deprecation_wrapper.py:119] From /home/TensorFlow/models/research/object_detection/core/losses.py:183: The name tf.losses.Reduction is deprecated. Please use tf.compat.v1.losses.Reduction instead.\n",
      "\n",
      "W1008 14:43:44.385448 140396949854016 deprecation.py:323] From /home/TensorFlow/models/research/object_detection/core/losses.py:350: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1008 14:43:45.763890 140396949854016 deprecation_wrapper.py:119] From /home/TensorFlow/models/research/object_detection/model_lib.py:372: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
      "\n",
      "W1008 14:43:45.769816 140396949854016 deprecation_wrapper.py:119] From /home/TensorFlow/models/research/object_detection/builders/optimizer_builder.py:58: The name tf.train.MomentumOptimizer is deprecated. Please use tf.compat.v1.train.MomentumOptimizer instead.\n",
      "\n",
      "W1008 14:43:45.769969 140396949854016 deprecation_wrapper.py:119] From /home/TensorFlow/models/research/object_detection/model_lib.py:400: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "W1008 14:43:50.919765 140396949854016 deprecation_wrapper.py:119] From /home/TensorFlow/models/research/object_detection/model_lib.py:503: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "I1008 14:43:51.247378 140396949854016 estimator.py:1147] Done calling model_fn.\n",
      "I1008 14:43:51.248342 140396949854016 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n",
      "I1008 14:43:54.608806 140396949854016 monitored_session.py:240] Graph was finalized.\n",
      "2019-10-08 14:43:54.609046: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2019-10-08 14:43:54.614748: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
      "2019-10-08 14:43:54.725787: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-10-08 14:43:54.726330: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1988d780 executing computations on platform CUDA. Devices:\n",
      "2019-10-08 14:43:54.726342: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1\n",
      "2019-10-08 14:43:54.756267: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3000000000 Hz\n",
      "2019-10-08 14:43:54.756556: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1988cc20 executing computations on platform Host. Devices:\n",
      "2019-10-08 14:43:54.756570: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2019-10-08 14:43:54.756885: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-10-08 14:43:54.757330: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
      "name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.721\n",
      "pciBusID: 0000:01:00.0\n",
      "2019-10-08 14:43:54.758701: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
      "2019-10-08 14:43:54.788940: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
      "2019-10-08 14:43:54.803053: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
      "2019-10-08 14:43:54.807869: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
      "2019-10-08 14:43:54.846974: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
      "2019-10-08 14:43:54.873370: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
      "2019-10-08 14:43:54.933224: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
      "2019-10-08 14:43:54.933399: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-10-08 14:43:54.934105: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-10-08 14:43:54.934671: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
      "2019-10-08 14:43:54.935214: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
      "2019-10-08 14:43:54.936429: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2019-10-08 14:43:54.936444: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
      "2019-10-08 14:43:54.936448: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
      "2019-10-08 14:43:54.937044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-10-08 14:43:54.937641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-10-08 14:43:54.939049: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10479 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\n",
      "2019-10-08 14:43:58.915000: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
      "I1008 14:43:58.958631 140396949854016 session_manager.py:500] Running local_init_op.\n",
      "I1008 14:43:59.230596 140396949854016 session_manager.py:502] Done running local_init_op.\n",
      "I1008 14:44:08.993339 140396949854016 basic_session_run_hooks.py:606] Saving checkpoints for 0 into /home/FasterRCNN_inV2_pigeon_test/training/model.ckpt.\n",
      "2019-10-08 14:44:18.201841: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
      "2019-10-08 14:44:22.683623: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
      "2019-10-08 14:44:26.973314: W tensorflow/core/common_runtime/bfc_allocator.cc:237] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.83GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2019-10-08 14:44:27.379317: W tensorflow/core/common_runtime/bfc_allocator.cc:237] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.55GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2019-10-08 14:44:27.385355: W tensorflow/core/common_runtime/bfc_allocator.cc:237] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.50GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2019-10-08 14:44:27.533015: W tensorflow/core/common_runtime/bfc_allocator.cc:237] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.59GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "I1008 14:44:28.532482 140396949854016 basic_session_run_hooks.py:262] loss = 1.6793635, step = 0\n",
      "2019-10-08 14:44:48.489041: W tensorflow/core/framework/allocator.cc:107] Allocation of 99532800 exceeds 10% of system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-08 14:44:48.519137: W tensorflow/core/framework/allocator.cc:107] Allocation of 99532800 exceeds 10% of system memory.\n",
      "2019-10-08 14:44:58.587971: W tensorflow/core/framework/allocator.cc:107] Allocation of 99532800 exceeds 10% of system memory.\n",
      "2019-10-08 14:44:58.607695: W tensorflow/core/framework/allocator.cc:107] Allocation of 99532800 exceeds 10% of system memory.\n",
      "2019-10-08 14:44:58.679837: W tensorflow/core/framework/allocator.cc:107] Allocation of 99532800 exceeds 10% of system memory.\n",
      "I1008 14:46:09.372285 140396949854016 basic_session_run_hooks.py:692] global_step/sec: 0.991667\n",
      "I1008 14:46:09.372865 140396949854016 basic_session_run_hooks.py:260] loss = 0.5987609, step = 100 (100.840 sec)\n",
      "I1008 14:47:46.859271 140396949854016 basic_session_run_hooks.py:692] global_step/sec: 1.02578\n",
      "I1008 14:47:46.859891 140396949854016 basic_session_run_hooks.py:260] loss = 0.4698762, step = 200 (97.487 sec)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python /home/TensorFlow/models/research/object_detection/model_main.py \\\n",
    "    --pipeline_config_path={pipeline_fname} \\\n",
    "    --model_dir='{model_dir}' \\\n",
    "    --alsologtostderr \\\n",
    "    --num_train_steps={num_steps} \\\n",
    "    --num_eval_steps={num_eval_steps}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KP-tUdtnRybs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint\t\t\t\t     model.ckpt-0.data-00000-of-00001\r\n",
      "events.out.tfevents.1570545832.c3c64ec9df27  model.ckpt-0.index\r\n",
      "graph.pbtxt\t\t\t\t     model.ckpt-0.meta\r\n"
     ]
    }
   ],
   "source": [
    "!ls '{model_dir}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_1Nrqw3nqnCh"
   },
   "outputs": [],
   "source": [
    "# Legacy way of training(also works).\n",
    "# !python /home/TensorFlow/models/research/object_detection/legacy/train.py --logtostderr --train_dir={model_dir} --pipeline_config_path={pipeline_fname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OmSESMetj1sa"
   },
   "source": [
    "## Exporting a Trained Inference Graph\n",
    "Once your training job is complete, you need to extract the newly trained inference graph, which will be later used to perform the object detection. This can be done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DHoP90pUyKSq"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "output_directory = './fine_tuned_model'\n",
    "\n",
    "lst = os.listdir(model_dir)\n",
    "lst = [l for l in lst if 'model.ckpt-' in l and '.meta' in l]\n",
    "steps=np.array([int(re.findall('\\d+', l)[0]) for l in lst])\n",
    "last_model = lst[steps.argmax()].replace('.meta', '')\n",
    "\n",
    "last_model_path = os.path.join(model_dir, last_model)\n",
    "print(last_model_path)\n",
    "!python /home/TensorFlow/models/research/object_detection/export_inference_graph.py \\\n",
    "    --input_type=image_tensor \\\n",
    "    --pipeline_config_path={pipeline_fname} \\\n",
    "    --output_directory={output_directory} \\\n",
    "    --trained_checkpoint_prefix='{last_model_path}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "usgBZvkz0nqD"
   },
   "outputs": [],
   "source": [
    "!ls {output_directory}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zip the output directory to drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "\n",
    "zip_save_path = '/content/drive/Shared drives/LIS Drone Scarecrow/Bird Detection/TensorFlow Model/trained_models'\n",
    "zip_model_num = last_model.rsplit('-', 1)[1]\n",
    "zip_date = now.strftime(\"%m%d%Y-%H%M%S\")\n",
    "zip_model_name = '{}/inference_graph_{}_{}.zip'.format(zip_save_path, zip_model_num, zip_date)\n",
    "!zip -r '{zip_model_name}' {output_directory}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p09AOThWkaQv"
   },
   "source": [
    "## Download the model `.pb` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CnDo1lonKgFr"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "pb_fname = os.path.join(os.path.abspath(output_directory), \"frozen_inference_graph.pb\")\n",
    "assert os.path.isfile(pb_fname), '`{}` not exist'.format(pb_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lHqWkLBINYoI"
   },
   "outputs": [],
   "source": [
    "!ls -alh {pb_fname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FIqnjbWYsuQw"
   },
   "source": [
    "### Option1 : upload the `.pb` file to your Google Drive\n",
    "Then download it from your Google Drive to local file system.\n",
    "\n",
    "During this step, you will be prompted to enter the token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hAqyASIJqjae"
   },
   "outputs": [],
   "source": [
    "# Install the PyDrive wrapper & import libraries.\n",
    "# This only needs to be done once in a notebook.\n",
    "!pip install -U -q PyDrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "\n",
    "# Authenticate and create the PyDrive client.\n",
    "# This only needs to be done once in a notebook.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "fname = os.path.basename(pb_fname)\n",
    "# Create & upload a text file.\n",
    "uploaded = drive.CreateFile({'title': fname})\n",
    "uploaded.SetContentFile(pb_fname)\n",
    "uploaded.Upload()\n",
    "print('Uploaded file with ID {}'.format(uploaded.get('id')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2FKFq8RXs6bs"
   },
   "source": [
    "### Option2 :  Download the `.pb` file directly to your local file system\n",
    "This method may not be stable when downloading large files like the model `.pb` file. Try **option 1** instead if not working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-bP0iMMnnr77"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download(pb_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MFyCeiBb9BbS"
   },
   "source": [
    "### Download the `label_map.pbtxt` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K1TbL6Ox8q6Z"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download(label_map_pbtxt_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iUmAo9foa1xq"
   },
   "source": [
    "### Download the modified pipline file\n",
    "If you plan to use OpenVINO toolkit to convert the `.pb` file to inference faster on Intel's hardware (CPU/GPU, Movidius, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pql2QpemazE1"
   },
   "outputs": [],
   "source": [
    "files.download(pipeline_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w1AgBj1l0v_W"
   },
   "outputs": [],
   "source": [
    "# !tar cfz fine_tuned_model.tar.gz fine_tuned_model\n",
    "# from google.colab import files\n",
    "# files.download('fine_tuned_model.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mz1gX19GlVW7"
   },
   "source": [
    "## Run inference test\n",
    "Test with images in repository `TF_object_detection/test` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pzj9A4e5mj5l"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
    "PATH_TO_CKPT = pb_fname\n",
    "\n",
    "# List of the strings that is used to add correct label for each box.\n",
    "PATH_TO_LABELS = label_map_pbtxt_fname\n",
    "\n",
    "# If you want to test the code with your images, just add images files to the PATH_TO_TEST_IMAGES_DIR.\n",
    "PATH_TO_TEST_IMAGES_DIR =  os.path.join(repo_dir_path, \"test\")\n",
    "\n",
    "assert os.path.isfile(pb_fname)\n",
    "assert os.path.isfile(PATH_TO_LABELS)\n",
    "TEST_IMAGE_PATHS = glob.glob(os.path.join(PATH_TO_TEST_IMAGES_DIR, \"*.*\"))\n",
    "assert len(TEST_IMAGE_PATHS) > 0, 'No image found in `{}`.'.format(PATH_TO_TEST_IMAGES_DIR)\n",
    "print(TEST_IMAGE_PATHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CG5YUMdg1Po7"
   },
   "outputs": [],
   "source": [
    "%cd /home/TensorFlow/models/research/object_detection\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# This is needed since the notebook is stored in the object_detection folder.\n",
    "sys.path.append(\"..\")\n",
    "from object_detection.utils import ops as utils_ops\n",
    "\n",
    "\n",
    "# This is needed to display the images.\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from object_detection.utils import label_map_util\n",
    "\n",
    "from object_detection.utils import visualization_utils as vis_util\n",
    "\n",
    "\n",
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "    od_graph_def = tf.GraphDef()\n",
    "    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
    "        serialized_graph = fid.read()\n",
    "        od_graph_def.ParseFromString(serialized_graph)\n",
    "        tf.import_graph_def(od_graph_def, name='')\n",
    "\n",
    "\n",
    "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
    "categories = label_map_util.convert_label_map_to_categories(\n",
    "    label_map, max_num_classes=num_classes, use_display_name=True)\n",
    "category_index = label_map_util.create_category_index(categories)\n",
    "\n",
    "\n",
    "def load_image_into_numpy_array(image):\n",
    "    (im_width, im_height) = image.size\n",
    "    return np.array(image.getdata()).reshape(\n",
    "        (im_height, im_width, 3)).astype(np.uint8)\n",
    "\n",
    "# Size, in inches, of the output images.\n",
    "IMAGE_SIZE = (12, 8)\n",
    "\n",
    "\n",
    "def run_inference_for_single_image(image, graph):\n",
    "    with graph.as_default():\n",
    "        with tf.Session() as sess:\n",
    "            # Get handles to input and output tensors\n",
    "            ops = tf.get_default_graph().get_operations()\n",
    "            all_tensor_names = {\n",
    "                output.name for op in ops for output in op.outputs}\n",
    "            tensor_dict = {}\n",
    "            for key in [\n",
    "                'num_detections', 'detection_boxes', 'detection_scores',\n",
    "                'detection_classes', 'detection_masks'\n",
    "            ]:\n",
    "                tensor_name = key + ':0'\n",
    "                if tensor_name in all_tensor_names:\n",
    "                    tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n",
    "                        tensor_name)\n",
    "            if 'detection_masks' in tensor_dict:\n",
    "                # The following processing is only for single image\n",
    "                detection_boxes = tf.squeeze(\n",
    "                    tensor_dict['detection_boxes'], [0])\n",
    "                detection_masks = tf.squeeze(\n",
    "                    tensor_dict['detection_masks'], [0])\n",
    "                # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
    "                real_num_detection = tf.cast(\n",
    "                    tensor_dict['num_detections'][0], tf.int32)\n",
    "                detection_boxes = tf.slice(detection_boxes, [0, 0], [\n",
    "                                           real_num_detection, -1])\n",
    "                detection_masks = tf.slice(detection_masks, [0, 0, 0], [\n",
    "                                           real_num_detection, -1, -1])\n",
    "                detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "                    detection_masks, detection_boxes, image.shape[0], image.shape[1])\n",
    "                detection_masks_reframed = tf.cast(\n",
    "                    tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
    "                # Follow the convention by adding back the batch dimension\n",
    "                tensor_dict['detection_masks'] = tf.expand_dims(\n",
    "                    detection_masks_reframed, 0)\n",
    "            image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
    "\n",
    "            # Run inference\n",
    "            output_dict = sess.run(tensor_dict,\n",
    "                                   feed_dict={image_tensor: np.expand_dims(image, 0)})\n",
    "\n",
    "            # all outputs are float32 numpy arrays, so convert types as appropriate\n",
    "            output_dict['num_detections'] = int(\n",
    "                output_dict['num_detections'][0])\n",
    "            output_dict['detection_classes'] = output_dict[\n",
    "                'detection_classes'][0].astype(np.uint8)\n",
    "            output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
    "            output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
    "            if 'detection_masks' in output_dict:\n",
    "                output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
    "    return output_dict\n",
    "\n",
    "\n",
    "for image_path in TEST_IMAGE_PATHS:\n",
    "    image = Image.open(image_path)\n",
    "    # the array based representation of the image will be used later in order to prepare the\n",
    "    # result image with boxes and labels on it.\n",
    "    image_np = load_image_into_numpy_array(image)\n",
    "    # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "    image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "    # Actual detection.\n",
    "    output_dict = run_inference_for_single_image(image_np, detection_graph)\n",
    "    # Visualization of the results of a detection.\n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        image_np,\n",
    "        output_dict['detection_boxes'],\n",
    "        output_dict['detection_classes'],\n",
    "        output_dict['detection_scores'],\n",
    "        category_index,\n",
    "        instance_masks=output_dict.get('detection_masks'),\n",
    "        use_normalized_coordinates=True,\n",
    "        line_thickness=8)\n",
    "    plt.figure(figsize=IMAGE_SIZE)\n",
    "    plt.imshow(image_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GStNeHWPkTcN"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "tensorflow-object-detection-training-colab.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
